{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary 2020/08 - Thesis of Ning Yue - Goal: Implementation a useful agent in scml\n",
    "\n",
    "### Second step - Implement a Deep Reinforcement Learning Negotiator in Negmas Simulator\n",
    "\n",
    "Firstly, Yasser and Tim, Sorry for this too later Summary.\n",
    "\n",
    "Due to too many reasons, the work are going too slowly. \n",
    "\n",
    "### Background of this Summary\n",
    "\n",
    "This Summary obtains mainly two parts work.\n",
    "\n",
    "1. The preparation\n",
    "    - Read the paper, get the theory knowledge\n",
    "    - Try to implemet two relative project.\n",
    "2. Implement a special Environment for developing a DRL-Negotiator based on Gym and Negmas\n",
    "    - Implement a Negotiation Environment\n",
    "    - Implement a Negotiation Game\n",
    "    - Implemet a DRL-Negotiator \n",
    "    - Evaluate a result\n",
    "\n",
    "My idea is: Before i give you Summary, i need to get some readable result.\n",
    "\n",
    "So after i have already read many relative papers and projects, i did not send you email immediately.\n",
    "\n",
    "I try to understand these by myself. But naturally i have a lot of questions. \n",
    "\n",
    "So in this Summary i have wrote two parts: preparation(Normally i need to send to you last month) \n",
    "and Implement a special Environment based on Gym and Negmas for developing a DRL-Negotiator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepration\n",
    "\n",
    "#### Papers and Theory Knowledge\n",
    "\n",
    "1. (basic) from all of the papers that received from you, i got the basic Knowledge of negotiation, for example: issue, outcome, utiltiy function, negotiation mechanism, heuristic negotiation, pareto optimal,  The impact of time, Opponents model, Accepting Optimally, Alternating Offers Protocols and so on.\n",
    "\n",
    "2. I have searched some papers by myself. for example\n",
    "    - [Decoupling Negotiating Agents to Explore the Space of Negotiation Strategy](https://www.researchgate.net/publication/263157273)\n",
    "    - [RLBOA: A Modular Reinforcement Learning Framework for Autonomous Negotiating Agents](https://homepages.cwi.nl/~baarslag/pub/RLBOA-A_Modular_Reinforcement_Learning_Framework_for_Autonomous_Negotiating_Agents.pdf)\n",
    "\n",
    "        Above two papers are about Decoupling negotiation, implemented in [Genius](http://users.umiacs.umd.edu/~sarit/data/articles//linetal-CI.pdf), bidding strategy(propose in negmas), opponent model(), acceptance strategy(negmas.modeling.acceptance), all of these modul could be learned by Reinforcement learning.\n",
    "\n",
    "    - [Concurrent bilateral negotiation for open e-markets:the CONAN strategy](https://doi.org/10.1007/s10115-017-1125-2)\n",
    "    - [A Deep Reinforcement Learning Approach to Concurrent Bilateral Negotiation](https://www.researchgate.net/publication/338989771)\n",
    "    \n",
    "        From above two papers, i know, it is possible to use DRL to learn negotiation strategy in concurrent bilateral negotiation, also in simple bilateral negotiation using DRL is naturally possible. I refer to the ideas in these papers: observation space, reward, and action space\n",
    "    \n",
    "    - [Optimal Negotiation Strategies for Agents with Incomplete Information](https://www.researchgate.net/publication/221454311_Optimal_Negotiation_Strategies_for_Agents_with_Incomplete_Information)\n",
    "\n",
    "        From this paper can get the Negotiation Environments, and about impact of time in negotiation (Linear, Conceder, Boulware) and so on. Negotiation Environments is useful for definition the observation space in reinfocement learning. later i will check it. \n",
    "        \n",
    "    - [Multi-issue Bargaining with Deep Reinforcement Learning](https://arxiv.org/abs/2002.07788)\n",
    "        \n",
    "        I am reading it now, have not finished. I'm not sure if it's necessary. But i am sure, that is useful.\n",
    "    And some papers about multi Agents, reinfocement learning, machine learning.\n",
    "\n",
    "#### Two Project\n",
    "\n",
    "##### [RLBOA](https://homepages.cwi.nl/~baarslag/pub/RLBOA-A_Modular_Reinforcement_Learning_Framework_for_Autonomous_Negotiating_Agents.pdf) in negmas\n",
    "\n",
    "I have almost finished it. But not useful for my thesis, so when i know the basic framework, I have stopped working. I just want to learn how to write rl in decoupling negotiation framework, so if you need it, I can send to you.\n",
    "\n",
    "I have also found similar modul in negmas, Advanced Negotiation.\n",
    "\n",
    "##### [ANEGMA](https://www.researchgate.net/publication/338989771) in negmas\n",
    "\n",
    "A drl negotiator in concurrent bilatreal negotiation, firstly, i want to realize it in negmas, but spent too much time, so i have just extracted some useful information from it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a special Environment based on Gym and Negmas for developing a DRL-Negotiator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Setting\n",
    "\n",
    "SAOMechanism, All Negotiators are the sub-class of SAONegotiator\n",
    "\n",
    "#### Components\n",
    "\n",
    "1. DRL-Negotiator: MyNegotiator\n",
    "2. Negotiation Game: NegotiationGame\n",
    "3. Negotiation Env: AnegmaEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install negmas==0.7.0\n",
    "!pip install gym==0.17.2\n",
    "# Before install stable_baselines Need to install libopenmpi-dev: command 'sudo apt install libopenmpi-dev'\n",
    "!pip install mpi4py==3.0.3\n",
    "!pip install stable_baselines==2.10.0\n",
    "!pip install tensorflow==1.15.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implemet a DRL-Negotiator\n",
    "\n",
    "The main tasks of DRL Negotiator are as follows:\n",
    "\n",
    "1. Received the observed state after every step, and then send to DRL Algorithm\n",
    "2. Received the action decided by DRL Algorithm, and then send to Negmas Simulator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DRL-Negotiator!\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "from negmas import Action #  An action that an `Agent` can execute in a `World` through the `Simulator`\n",
    "from negmas import (\n",
    "                        Issue, \n",
    "                        MechanismState, # The mechanism state\n",
    "                        SAOMechanism, \n",
    "                        SAONegotiator,  \n",
    "                        MappingUtilityFunction, \n",
    "                        AspirationNegotiator,\n",
    "                        ResponseType,\n",
    "                    )\n",
    "    \n",
    "import random\n",
    "\n",
    "\n",
    "def my_utility_function(offer, time, rp, ip, t_end):\n",
    "    '''\n",
    "        my utility function, \n",
    "        consider both the outcome and time\n",
    "    '''\n",
    "    d_t = 0.6\n",
    "    # print(offer, time, ip, rp, t_end)\n",
    "    # print(((float(rp) - float(offer[0])) / (float(rp) - float(ip))) * (float(time) / float(t_end)) ** d_t)\n",
    "    return ((float(rp) - float(offer[0])) / (float(rp) - float(ip))) * (float(time) / float(t_end)) ** d_t\n",
    "\n",
    "class MyNegotiator(SAONegotiator):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"my_negotiator\")\n",
    "        self.current_action: Optional[ResponseType] = None\n",
    "        self.end_time = 1\n",
    "        self.initial_price = random.sample(range(300, 350), 1)[0]\n",
    "        self.reserved_price = random.sample(range(500, 550), 1)[0]\n",
    "        self.rational_proposal = True\n",
    "        self.random_proposal = False\n",
    "        # self.reserved_value = None\n",
    "   \n",
    "    def propose_(self, state: MechanismState) -> Optional[\"Outcome\"]:\n",
    "        \n",
    "        if not self._ami._mechanism._running:\n",
    "            return None\n",
    "        proposal = self.propose(state=state)\n",
    "        # import pdb;pdb.set_trace()\n",
    "        # never return a proposal that is greater than the reserved value\n",
    "        if self.rational_proposal:\n",
    "            utility = None\n",
    "            if proposal is not None and self._utility_function is not None:\n",
    "                utility = self.get_utility(proposal)\n",
    "                if utility is not None and utility < self.get_utility((self.reserved_price,)):\n",
    "                    print('utility is less than the utility of reserved price')\n",
    "                    # when the utility of proposaled offer is less than the utility of reserved price,\n",
    "                    # then this proposaled offer is useless\n",
    "                    self.checked_proposal = False\n",
    "                    return None\n",
    "\n",
    "            if utility is not None:\n",
    "                self.my_last_proposal = proposal\n",
    "                self._my_last_proposal = proposal \n",
    "                self.my_last_proposal_utility = utility\n",
    "                self._my_last_proposal_utility = utility\n",
    "        \n",
    "        return proposal\n",
    "        \n",
    "    def respond(self, state: MechanismState, offer: \"Outcome\") -> \"ResponseType\":\n",
    "        print(\"The response of my negotiator has been called\")\n",
    "        # if self.current_action == ResponseType.ACCEPT_OFFER:\n",
    "        #     import pdb;pdb.set_trace()\n",
    "        # print(self.current_action)\n",
    "        # if self.current_action == ResponseType.ACCEPT_OFFER:\n",
    "        #     import pdb;pdb.set_trace()\n",
    "\n",
    "        if isinstance(self.current_action, ResponseType):\n",
    "            return self.current_action\n",
    "    \n",
    "    def propose(self, state: MechanismState) -> Optional[\"Outcome\"]:\n",
    "        '''\n",
    "            Here define the function that how to propose,\n",
    "            just when the RepsonseType is REJECT_OFFER or the \n",
    "            offer from opponent is None, this function will be called!\n",
    "        '''\n",
    "        print(\"The propose of my negotiator has been called\")\n",
    "        print(self.current_action)\n",
    "        # self.checked_proposal = False\n",
    "        if isinstance(self.current_action, ResponseType) \\\n",
    "                and self.current_action == ResponseType.REJECT_OFFER:\n",
    "            def _propose():\n",
    "                '''\n",
    "                    TODO: calculate the new outcome\n",
    "                    initial: random propose, \n",
    "                    Improvement: the best outcome that has the best utility in current time\n",
    "                '''\n",
    "                if self.random_proposal:\n",
    "                    return self._ami.outcomes[random.randint(0, len(self._ami.outcomes))-1]\n",
    "                else:\n",
    "                    # TODO: Continuous action space\n",
    "                    # add one value, this is a Continuous action space, return Gaussian probability distribution,\n",
    "                    # and then sample a value from this disctribution DDPG\n",
    "                    # return self._ami.outcomes[random.randint(0, len(self._ami.outcomes))-1]\n",
    "                    if self.my_last_proposal:\n",
    "                        print('my last proposal has been called!', self.my_last_proposal[0] + 5)\n",
    "                        return self.my_last_proposal[0] + 5, \n",
    "                    else:\n",
    "                        print('initial price')\n",
    "                        return self.initial_price + 1,\n",
    "            \n",
    "            # print(\"_propose called\")\n",
    "            self.checked_proposal = True \n",
    "            return _propose()\n",
    "        else:\n",
    "            # not offer received from opponent, so create random offer.\n",
    "            self.checked_proposal = True\n",
    "            return self._ami.outcomes[random.randint(0, len(self._ami.outcomes))-1]\n",
    "    \n",
    "    def get_last_proposal(self):\n",
    "        return self.my_last_proposal\n",
    "\n",
    "    def get_current_action(self):\n",
    "        return self.current_action\n",
    "        \n",
    "    def set_current_action(self, action: int):\n",
    "        # import pdb;pdb.set_trace()\n",
    "        # print(\"set current action called\", ResponseType(action))\n",
    "        self.current_action = ResponseType(action)\n",
    "    \n",
    "    def get_utility(self, offer: Optional[\"outcome\"]):\n",
    "        return self.ufun(offer, self.get_time(), self.get_reserved_price(), \\\n",
    "                    self.get_initial_price(), self.get_maximum_end_time())\n",
    "        \n",
    "    def get_maximum_end_time(self):\n",
    "        '''\n",
    "            Get the end time of my negotiator\n",
    "        '''\n",
    "        if hasattr(self, 'end_time'):\n",
    "            return self.end_time\n",
    "        else:\n",
    "            return float(\"inf\")\n",
    "\n",
    "    def get_initial_price(self):\n",
    "        '''\n",
    "            initial price which my negotiator offer\n",
    "        '''\n",
    "        return self.initial_price\n",
    "        \n",
    "    def get_reserved_price(self):\n",
    "        '''\n",
    "            Maximum price which my negotiator can offer to opponent\n",
    "        '''\n",
    "        # import pdb;pdb.set_trace()\n",
    "        return self.reserved_price\n",
    "    \n",
    "    def get_time(self):\n",
    "        # import pdb;pdb.set_trace()\n",
    "        # print(self._ami._mechanism.id)\n",
    "        # print(self._ami._mechanism.time)\n",
    "        return self._ami._mechanism.time\n",
    "    \n",
    "    def get_obs(self, x_best) -> np.array:\n",
    "        '''\n",
    "        >>> get_obs(550)\n",
    "        np.array([550, 150.0, 335, 522])\n",
    "        '''\n",
    "        x_best = x_best\n",
    "        # import pdb;pdb.set_trace()\n",
    "        time_left = self.get_maximum_end_time() - self.get_time()\n",
    "        ip_my = self.get_initial_price()\n",
    "        rp_my = self.get_reserved_price()\n",
    "        \n",
    "        result = [x_best, time_left, ip_my, rp_my]\n",
    "        scale_factory =  1 # scale inputs to be in the order of magnitude of 100 for neural network\n",
    "        return np.array(result) / scale_factory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negotiation Game\n",
    "\n",
    "Need to realize a Negotiation Game based on Negmas, The following functions are needed to realize in this Game:\n",
    "\n",
    "1. The logic of game, manager the negotiator\n",
    "2. base on the result of every step to define the reward, very important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Bilateral, single issue Game\n",
    "'''\n",
    "import random\n",
    "# from negmas_negotiation.negotiators import MyNegotiator, my_utility_function\n",
    "from negmas import (MappingUtilityFunction, AspirationNegotiator, Issue, SAOMechanism, ResponseType)\n",
    "from typing import Optional\n",
    "\n",
    "class NegotiationGame:\n",
    "    \n",
    "    def __init__(self, np_random=None):\n",
    "        self.np_random = None\n",
    "        \n",
    "        self.life = None\n",
    "        self._session_data = None\n",
    "        self._session = None\n",
    "        self.reset()\n",
    "        \n",
    "    def add_negotiators(self, negotiators: dict = None):\n",
    "        '''\n",
    "            Negotiators = {\"my_negotiator\": , \"opponent_negotiator\":}\n",
    "            Add my negotiator and opponent negotiator\n",
    "        '''\n",
    "        for _ in negotiators:\n",
    "            if 'my' in _:\n",
    "                params = {'ufun':my_utility_function}\n",
    "                self.add_my_negotiator(negotiators[_], params=params)\n",
    "            else:\n",
    "                self.add_opponent_negotiator(negotiators[_])\n",
    "            \n",
    "    \n",
    "    def add_my_negotiator(self, negotiator, params: dict=None):\n",
    "        self.my_negotiator = negotiator\n",
    "        self.my_negotiator_params = params\n",
    "        # import pdb;pdb.set_trace()\n",
    "        if not params:\n",
    "            self._session.add(self.my_negotiator, ufun=MappingUtilityFunction(lambda x: random.random() * x[0]))\n",
    "        else:\n",
    "            self._session.add(self.my_negotiator, ufun=self.my_negotiator_params['ufun'])\n",
    "    \n",
    "    def add_opponent_negotiator(self, negotiator, params:dict = None):\n",
    "        self.opponent_negotiator = negotiator\n",
    "        self.opponent_negotiator_params = params\n",
    "        if not params:\n",
    "            self._session.add(self.opponent_negotiator, ufun=MappingUtilityFunction(lambda x: random.random() * x[0]))\n",
    "        else:\n",
    "            self._session.add(self.opponent_negotiator, ufun=params['ufun'])\n",
    "        \n",
    "    def run(self):\n",
    "        self._session.run()\n",
    "    \n",
    "    def step(self, action:int=None):\n",
    "        '''\n",
    "            Can_Improv\n",
    "            Run the game session step by step. \n",
    "            Return reward, now just based on the agreement, \n",
    "            can add the time paramters\n",
    "        '''\n",
    "        self.action = action\n",
    "        self.my_negotiator.set_current_action(self.action)\n",
    "        # print('session id', self._session.id)\n",
    "        # print('session time', self._session.time)\n",
    "        result = self._session.step()\n",
    "        result = self._session.state\n",
    "        # if not self._session._running:\n",
    "        #     raise StopIteration\n",
    "        \n",
    "        # The logic of reward\n",
    "        # during classification, \n",
    "            # first condition: t<=t_end, Agreement, Ub（x, t）\n",
    "            # second condtion: t<=t_end, No Deal\n",
    "            # third  condition: if a_t = Counter-offer, a_t' = ResponseType.REJECT_OFFER\n",
    "            # otherwise: 0\n",
    "        # during regression, if the a_t is counter-offer, ResponseTyep.REJECT_OFFER in negmas\n",
    "            # first condition: t<=t_end, x<= all_i belong to O_t, Ub(x, t)\n",
    "            # second condtion: t<=t_end, x>all_i belong to O_t, -1\n",
    "            # last condtion: otherwise, 0\n",
    "        # TODO: self.my_negotiator.propose_offer\n",
    "        reward = 0\n",
    "        print('x best is:', self.get_x_best())\n",
    "        # print(self.my_negotiator.checked_proposal, self.my_negotiator.my_last_proposal_utility)\n",
    "        if self.my_negotiator.get_time()<=self.my_negotiator.get_maximum_end_time():\n",
    "            # Agreement\n",
    "            if self.my_negotiator.current_action == ResponseType.ACCEPT_OFFER:\n",
    "                if result.agreement:\n",
    "                    # if result.agreement[0] < self.my_negotiator.get_reserved_price():\n",
    "                    # import pdb;pdb.set_trace()\n",
    "                    print('reward agreement!', result.agreement)\n",
    "                    print('rp, ip', self.my_negotiator.get_reserved_price(), self.my_negotiator.get_initial_price())\n",
    "                    reward = self.my_negotiator.get_utility(result.agreement)\n",
    "                    reward += 0.5 # extra reward for negotiator when get a agreement\n",
    "                    # print('reward agreement!')\n",
    "                    # else:\n",
    "                    #     reward = -1\n",
    "                else:\n",
    "                    # import pdb;pdb.set_trace()\n",
    "\n",
    "                    print('reward 0, when the offer is None, but set the action as ACCEPT OFFER!')\n",
    "                    reward = 0\n",
    "            # at = Counter-Offer\n",
    "            elif self.my_negotiator.current_action == ResponseType.REJECT_OFFER:\n",
    "                if self.my_negotiator.checked_proposal \\\n",
    "                            and self.my_negotiator._my_last_proposal_utility \\\n",
    "                                and self.my_negotiator._my_last_proposal[0] <= self.get_x_best():\n",
    "                    # useful proposal\n",
    "                    reward = self.my_negotiator._my_last_proposal_utility\n",
    "                    print('reward reject offer new offer')\n",
    "                    self.my_negotiator.checked_proposal = False\n",
    "                    self.my_negotiator._my_last_proposal = None\n",
    "                    self.my_negotiator._my_last_proposal_utility = None\n",
    "                else:\n",
    "                    # if proposal is greater than x best, then the reward is -1\n",
    "                    print('reward reject offer the new offer is greater than x best or checked proposal is False: \\\n",
    "                                utility is less than the utility of reserved price', self.get_x_best())\n",
    "                    reward = -1\n",
    "                # reset all checked value and _last_proposal was already used\n",
    "\n",
    "            elif isinstance(self.my_negotiator.current_action, ResponseType) and \\\n",
    "                    (self.my_negotiator.current_action == ResponseType.END_NEGOTIATION or  \\\n",
    "                        self.my_negotiator.current_action == ResponseType.NO_RESPONSE):\n",
    "                print('reward No Deal')\n",
    "                reward = -1\n",
    "            else:\n",
    "                print('reward time is less than max end time, but the Response is Wait')\n",
    "                # Here need give a penalty, not just reward 0\n",
    "                # reward = -(self.my_negotiator.get_time() / self.my_negotiator.get_maximum_end_time()) ** 0.3\n",
    "                reward = 0\n",
    "        else:\n",
    "            # if the time is greater than t end, then the reward is zero.\n",
    "            print(\"reward time is greater than maximum end time\")\n",
    "            reward = 0\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def get_life(self):\n",
    "        return self._session.running\n",
    "        \n",
    "    def get_issues(self):\n",
    "        return self._session.issues\n",
    "    \n",
    "    def get_time(self):\n",
    "        '''\n",
    "            Wall time\n",
    "        '''\n",
    "        return self._session.state.time\n",
    "    \n",
    "    def get_relative_time(self):\n",
    "        '''\n",
    "            [0, 1]\n",
    "        '''\n",
    "        return self._session.state.relative_time\n",
    "    \n",
    "    def get_state(self):\n",
    "        # import pdb;pdb.set_trace()\n",
    "        return self._session.state\n",
    "    \n",
    "    def get_utility_space(self):\n",
    "        '''\n",
    "            TODO:\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    def get_outcome_space(self):\n",
    "        return self._session.outcomes\n",
    "    \n",
    "    \n",
    "    def get_x_best(self):\n",
    "\n",
    "        '''\n",
    "            TODO: The best offer that opponent has offered based self._session.history\n",
    "        '''\n",
    "        k = [_.current_offer for _ in self.get_history() if _.current_proposer \\\n",
    "                    and 'opponent_negotiator' in _.current_proposer] \n",
    "        if k:\n",
    "            return min(k)[0]\n",
    "        else:\n",
    "            return float('inf') / 1\n",
    "            \n",
    "    def get_history(self):\n",
    "        return self._session.history\n",
    "        \n",
    "    def get_observation(self):\n",
    "        '''\n",
    "            Get observation state\n",
    "            Important\n",
    "            [350$, 20s, 340$, 530$] -> [X_best, time_left, IP, RP]\n",
    "        '''\n",
    "        x_best = self.get_x_best()\n",
    "        return self.my_negotiator.get_obs(x_best)\n",
    "    \n",
    "    def reset(self):\n",
    "        self._session = SAOMechanism(issues=[Issue((300, 550))], n_steps=50)\n",
    "        negotiators = {\"my_negotiator\": MyNegotiator(),\\\n",
    "                    'opponent_negotiator': AspirationNegotiator(name=\"opponent_negotiator\")}\n",
    "        self.add_negotiators(negotiators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negotiation Environment\n",
    "\n",
    "I have searched two projects which implement framework for researching reinforcement learning.\n",
    "\n",
    "1. **open_spiel**, develped by deepmind, algorithm implemented with C++.\n",
    "2. **gym**, develped by openai, pure python.\n",
    "\n",
    "All of above projects are very useful when people want to research reinforcement learning.\n",
    "\n",
    "But now i just want to realize  DRL-Negotiator.\n",
    "\n",
    "**gym** is easier for me to combine the different reinforcement learning algorithm \n",
    "and Negotiator in Negmas simulator.\n",
    "\n",
    "So at the end, i have selected **gym** to implement it.\n",
    "\n",
    "I call the Environment as **AnegmasEnv**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from gym import spaces\n",
    "from typing import Optional\n",
    "from negmas import (MechanismState, ResponseType)\n",
    "# from .negotiation_games import NegotiationGame\n",
    "from gym.utils import seeding\n",
    "\n",
    "class AnegmaEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(AnegmaEnv, self).__init__()\n",
    "        self.game = None\n",
    "        \n",
    "                \n",
    "        # Define the action space and observation space \n",
    "        self.observation_space = self._observ_space()\n",
    "        self.action_space = self._action_space() # convert the mechanism state to observation space\n",
    "        \n",
    "        # set up the game\n",
    "        self.game = NegotiationGame()\n",
    "        self.mechanism_state: Optional[MechanismState] = None\n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        # if call up the seed function, then set up the game with seed\n",
    "        self.game = NegotiationGame(np_random=self.np_random)\n",
    "        return [seed]\n",
    "    \n",
    "    def _observ_space(self):\n",
    "        '''\n",
    "        Observeration space of Negotiator,\n",
    "        state: (\"Xbest\":, \"Tleft\":, \"IP_my\", \"RP_my\")\n",
    "        X_best: best offer made by opponent,\n",
    "        T_left: time left for my negotiator to reach t_end after the last action of opponent negotiator\n",
    "        IP_my: intial price, minium price which my negotiator can offer at the start of negotiation\n",
    "        RP_my: reserved price, maximum price which my negotiator can offer to opponent\n",
    "\n",
    "        negotiation scenario:\n",
    "        First step just consider a simple negotiation scenario, in other words, a simple negotiation settings\n",
    "            Issue = {'laptop_price'}\n",
    "            IP_b = [300 - 350] # the range of intial price of negotiator\n",
    "            RP_b = [500 - 550] # the range of reserved price of my negotiator\n",
    "            IP_s = [500 - 550] # the range of intial price of opponent, here not implemented, just use default setting of AspirationNegotiator\n",
    "            RP_s = [300 - 350] # the range of reserved price of opponent, here not implemented, just use default setting of Aspirationnegotiator\n",
    "            MD   = [2] # numbers of negotiators\n",
    "            MR   = [1:1] # the ratio of my negotiator and opponent negotiator\n",
    "            t_end = [0 - 210s]\n",
    "            random sample a value from above range\n",
    "            for example\n",
    "            t_end = 1s,\n",
    "            so the observ space is [(300 - 550), (0 - 1), (300-350), (500-550)]\n",
    "        '''\n",
    "        observ_space = spaces.Box(low=np.array([300, 0, 300, 500]), high=np.array([550, 1, 350, 550]), dtype=np.int)\n",
    "        return observ_space\n",
    "    \n",
    "    \n",
    "    def _action_space(self):\n",
    "        \n",
    "        '''\n",
    "        Action Space, Discrete space.\n",
    "        actions that agent can select are [0, 1, 2, 3, 4]\n",
    "        0. ResponseType.REJECT_OFFER, will be gived a chance to propose new offer.\n",
    "        1. ResponseType.ACCEPT_OFFER\n",
    "        2. ResponseType.END_NEGOTIATION\n",
    "        3. ResponseType.NO_RESPONSE, refuse to offer.\n",
    "        4. ResponseType.WAIT\n",
    "        \n",
    "        When call function respond_ and firstly get the action ResponseType.REJECT_OFFER, then will call the function proposal_ to \n",
    "        get the new offer.\n",
    "        \n",
    "        The details are described in the paper: `A Deep Reinforcement learning approach to concurrent negotiation`\n",
    "        '''\n",
    "        \n",
    "        action_space = spaces.Discrete(5)\n",
    "        return action_space\n",
    "    \n",
    "    def step(self, action: int=None):\n",
    "        '''\n",
    "            Gym env, step function!\n",
    "        '''\n",
    "        print('\\033[31m action comes from DRL is', ResponseType(action))\n",
    "        print('\\033[0m')\n",
    "        print('n_steps is', self.n_step)\n",
    "        done = False\n",
    "        self.n_step += 1\n",
    "        self.time = self.game.get_time()\n",
    "        \n",
    "        # get the reward, and run one step in game\n",
    "        reward = self.game.step(action)\n",
    "        # reward = 0\n",
    "        \n",
    "        obs = self.get_obs()\n",
    "        \n",
    "        if self.n_step >= self.game._session.n_steps \\\n",
    "            or self.time >=self.game._session.time_limit:\n",
    "            done = True\n",
    "        \n",
    "        if not self.game.get_life():\n",
    "            done = True\n",
    "        \n",
    "        # if self.game.my_negotiator.get_current_action() == ResponseType.END_NEGOTIATION:\n",
    "        #     # if my negotiator proposed a not rational offer, then end the negotiation, \n",
    "        #     # but the negmas simulator do not know this response, so manual finished this episode.\n",
    "        #     done = True\n",
    "        \n",
    "        # determine which info Need to be added!\n",
    "        info = {\n",
    "            'state': self.game.get_state()\n",
    "        }\n",
    "        # print(obs, reward, done, info)\n",
    "        print(reward)\n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def init_game_state(self):\n",
    "        self.n_step = 0\n",
    "        self.time = 0\n",
    "        self.game.reset()\n",
    "    \n",
    "    def get_obs(self):\n",
    "        '''\n",
    "            Get obs state\n",
    "        '''\n",
    "        return self.game.get_observation()\n",
    "    \n",
    "    def reset(self):\n",
    "        '''\n",
    "            Reset the state of the environment to an initial state,\n",
    "            When the value of done from the result of function step() is True, \n",
    "            this function will be called\n",
    "            create a new game\n",
    "        '''\n",
    "        # print(\"reset has been called\")\n",
    "        self.init_game_state()\n",
    "        # print(id(self.game), id(self.game._session))\n",
    "        return self.get_obs() # the initial observation space\n",
    "    \n",
    "    def render(self, mode='human', close=False):\n",
    "        '''\n",
    "            Render the environment to the screen\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Train with DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from stable_baselines import DQN\n",
    "from stable_baselines.ppo1 import PPO1\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines import logger\n",
    "from stable_baselines.common.callbacks import EvalCallback\n",
    "from stable_baselines import results_plotter\n",
    "from stable_baselines.bench import Monitor\n",
    "\n",
    "NUM_TIMESTEPS = int(1000)\n",
    "# The seed for generating the random number\n",
    "SEED = 721\n",
    "# Setting for Evaluation\n",
    "EVAL_FREQ = 100\n",
    "EVAL_EPISODES = 2\n",
    "# LOGDIR \n",
    "LOGDIR = \"Test_dqn\"\n",
    "\n",
    "logger.configure(folder=LOGDIR)\n",
    "\n",
    "env=AnegmaEnv()\n",
    "env = Monitor(env, LOGDIR)\n",
    "env.seed(SEED)\n",
    "# import pdb;pdb.set_trace()\n",
    "# model = PPO1(MlpPolicy, env, timesteps_per_actorbatch=40, clip_param=0.2, entcoeff=0.0, optim_epochs=10,\n",
    "#                  optim_stepsize=3e-4, optim_batchsize=2, gamma=0.99, lam=0.95, schedule='linear', verbose=2)\n",
    "\n",
    "model = DQN('MlpPolicy', env, learning_rate=1e-3, prioritized_replay=True, verbose=1)\n",
    "eval_callback = EvalCallback(env, best_model_save_path=LOGDIR, log_path=LOGDIR, eval_freq=EVAL_FREQ, n_eval_episodes=EVAL_EPISODES)\n",
    "\n",
    "model.learn(total_timesteps=NUM_TIMESTEPS, callback=eval_callback)\n",
    "\n",
    "model.save(os.path.join(LOGDIR, \"final_model\")) # probably never get to this point.\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load and show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TIMESTEPS = int(1000)\n",
    "LOGDIR = \"Test_dqn\"\n",
    "results_plotter.plot_results([LOGDIR], NUM_TIMESTEPS, results_plotter.X_TIMESTEPS, \"DQN Negotiation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the result that i have runned 5000000 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTENTION: \n",
    "# If you run by yourself, will take a moment\n",
    "# or can just skip this cell, see the result i have shown in next cell\n",
    "\n",
    "from stable_baselines import results_plotter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NUM_TIMESTEPS = 5000000\n",
    "LOGDIR = \"dqn\"\n",
    "results_plotter.plot_results([LOGDIR], NUM_TIMESTEPS, results_plotter.X_TIMESTEPS, \"DQN Negotiation\")\n",
    "plt.savefig('dqn_result.png', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result of 5000000 steps\n",
    "\n",
    "The blue circle: monitor.csv, the sum reward of every episode: means one finished negotiation session.\n",
    "\n",
    "The blue line:  progress.csv, mean 100 episode reward.\n",
    "\n",
    "From the figure, to a certain extent, the DRL-Negotiator learn a strategy \n",
    "to reponse the opponent based on the state that i have defined in source code.\n",
    "\n",
    "Problem:\n",
    "\n",
    "1. spent too much time to learn, only after 2000000 steps, could see the slight change.\n",
    "2. oscillation: Reward suddenly went very low \n",
    "\n",
    "![result after 5000000 steps](./dqn_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions and future Work in 2020/08 - 2020/09\n",
    "\n",
    "Future work\n",
    "\n",
    "1. Problem in Result of 5000000 steps that i have mentioned, I will try to use another DRL algorithm, for example, trop, ppo and so on, at the end compare the result.\n",
    "2. Use multi processing to learn, accelerate learn\n",
    "3. Change and refine the observation space, action space, reward\n",
    "4. Different negotiation settings, different negotiators and so on.\n",
    "5. Make the code more robust and suitable for more scenarios: There are too many fixed parameters now  \n",
    "\n",
    "Questions:\n",
    "\n",
    "1. I have seen too few papers using DRL to learn how to negotiate. If you have, could you send to me? It will be very useful.\n",
    "2. Whether I need to analyze the scenario: multi issue, large scale numbers of negotiators, if it is necessarily, could you give some suggestions?\n",
    "3. Can simply transfer this negotiator to scml? I will check it.\n",
    "\n",
    "**Important Questions:** \n",
    "How to combine Acceptance Strategy and Bidding Strategy?\n",
    "\n",
    "When i use DRL to learn how to decide the next action, i have found an interesting situation\n",
    "\n",
    "1. Just consider the response action as ResponseType.ACCEPT_OFFER, ResponseType.REJECT_OFFER... here is simple,just set the action as discrete value.(Acceptance Strategy)\n",
    "2. But when the action is ResponseType.REJECT_OFFER, it is possible that my negotiator create a new offer. In this situation, how to decide the outcome is a important question.(Bidding Strategy)\n",
    "\n",
    "in machine learning, propose an outcome is a regression Problem, response is a classification problem. \n",
    "\n",
    "So my Questions is, if it is possibe, i create a new model, the input is the negotiation state, first solve the classification problem(response), if the action is ResponseType.REJECT_OFFER, then at the same time, this DRL model output an outcome (whether it is considered as continous action space?).\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "If you have any suggestions, you can send to me, I need all of them.\n",
    "\n",
    "I need more definite direction and goals. \n",
    "It may be better to determine the title of the paper. \n",
    "\n",
    "I feel more and more interesting in this direction and also find many problems in this field."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
